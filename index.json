[{"categories":["Copilot","Microsoft Search","Declarative Agents"],"contents":"Recently, I came across an interesting use case building an HR agent allowing employees to ask common questions about company internal policies. These policies were made available to Copilot through a custom Graph Connector. However, during our tests, we realized for some questions, Copilot was struggling answering correctly even though answers were present in FAQ documents alongside the policy itself. To improve answers accuracy, we had the idea to extract the content from these FAQs and leverage the QnA feature of Microsoft Search. This way, we\u0026rsquo;ve added them as an additional source for our agent through an API pluigin (by default QnAs aren\u0026rsquo;t crawl into the semantic index and can\u0026rsquo;t be used in answers).\nThis blog post describes how to use the Microsoft Graph API as a Copilot plugin in a declarative agent. We take a generic example of a agent only retrieving data from QnAs content defined in Microsoft Search using the /search/query endpoint of the Microsoft Graph API.\nThis technique works for any entity types in Microsoft Search (ex: listItem, bookmark, etc.). However I don\u0026rsquo;t recommend to use it to pull items information from SharePoint or OneDrive as it duplicates what copilot already does behind the scenes with the semantic index.\n The complete example is available on GitHub: https://github.com/FranckyC/copilot-pro-dev-samples/tree/main/samples/da-qna-graphapi-plugin\n 1. Get Microsoft Graph Open API spec file As plugins rely on Open API specifications, the first thing to do is to get the specification for the Microsoft Graph API. Despite the complete Open API specification exsits, for perfomrance reasons (the complete file is 34MB!), we start with a smaller Search.yml subset available from here: https://github.com/microsoftgraph/msgraph-sdk-powershell/tree/dev/openApiDocs/v1.0\nDownload that file into your agent solution for example in apis/search.yml.\n2. Extract the required operations from the specification Even with a subset, the API specification still contains a lot of information. In our case, we are only interested by the /search/query endpoint. We use the Hidi comand line tool to generate the Open API spec only for that specific endpoint by using the following command:\nhidi transform -d apis\\search.yml -f json -o apis\\graph_search.yaml -v OpenApi3_0 --op search_query --co Where:\n -d apis\\search.yml is the \u0026ldquo;raw\u0026rdquo; Open API to simplify. -o apis\\graph_search.yaml is the new Open API spec file we wnat to use in our plugin. -v OpenApi3_0 the version of OpenAI to use for the output file. --op search_query the operationId in the input file corresponding to the endpoint we want to extract.  Now we have or spec only for the required endpoint, it is time to generate the Copilot plugin information.\n3. Generate the Copilot plugin information To generate the plugin information that we will be used by the Copilot agent, we use the Kotia tool. If not installed already, install the Kiota extension for Visual Studio Code.\n Why using Kiota instead of the built-in Teams ToolKit plugin generation? The default Teams Toolkit plugin generation can struggle to generate plugins for complex property types like arrays or nested objects. Using Kiota will ensure \u0026gt; the plugin information is generated correctly.\n Then from the Kiota options, add a new API description and select \u0026ldquo;Browse path\u0026rdquo; and select the file graph_search.yml you generated from the previous step:\nIt should load all the available endpoints from the file. Add the endpoint (the \u0026lsquo;+\u0026rsquo; sign on the POST line), the click on \u0026ldquo;Generate\u0026rdquo;:\nSelect \u0026ldquo;Copilot Plugin\u0026rdquo; and give it a name, for instance msSearchPlugin:\nFor the folder, we recommend to generate file in a dedicated folder (ex: \u0026ldquo;plugins\u0026rdquo;):\nWe can now import the plugin with Teams Toolkit to integrate it with the declarative agent.\n4. Import the plugin with Microsoft Teams Toolkit In Visual Studio, in the Teams ToolKit options, use the Add Plugin option and select _Import an existing plugin. Select the mssearchplugin-apiplugin.json manifest file (.json) and the Open API specification (.yml) from the plugin information generated by Kiota. Finally, select the declarative agent manifest file to integrate the plugin.\nFrom here, you should see a ai-plugin.json and mssearchplugin-openapi.yml files generated under the \u0026ldquo;appPackage\u0026rdquo; folder:\nWarning\nBecause QnAs are only available in the /beta endpoint, you need to manually update the base URL in the mssearchplugin-openapi.yml:\nopenapi: 3.0.1 info:  title: Search - Subset - Subset  version: beta servers:  - url: https://graph.microsoft.com/beta/  description: Core  ... The Microsoft Graph API being protected by Entra ID, we need to specify authentication parameters to use it.\n5. Add OAuth2 authentication Open the mssearchplugin-openapi.yml file, look for the securitySchemes property and replace by the following (replace tenantid by your ID):\nsecuritySchemes:  azureaadv2:  type: oauth2  flows:  authorizationCode:  authorizationUrl: https://login.microsoftonline.com/tenantid/oauth2/v2.0/authorize  tokenUrl: https://login.microsoftonline.com/tenantid/oauth2/v2.0/token  scopes:  QnA.Read.All: Read QnAs from Microsoft Search Because the authentication to the API is performed through OAuth2, you need to first create a dedicated Entra ID application in your tenant and then register an OAuth connection in the Teams developer portal:\n Register an Entra ID application in Azure and add the API permissions QnA.Read.All (delegated). Add the https://teams.microsoft.com/api/platform/v1.0/oAuthRedirect as a redirect URL for web platform in the Authentication settings.  In the Teams developer portal, add a new OAuth client registration and register a new client with the following information:  App settings\n Registration name: MicrosoftSearch Base URL: https://graph.microsoft.com/beta (QnA are only usable through the beta endpoint) Restrict usage by org: My organization only Restrict usage by app: Any Teams app (When agent is deployed, use the Teams app ID).  OAuth settings\n Client ID: \u0026lt;the entra ID application ID\u0026gt; Client secret: \u0026lt;the Entra ID application secret\u0026gt; Authorization endpoint: https://login.microsoftonline.com/tenantid/oauth2/v2.0/authorize Token endpoint: https://login.microsoftonline.com/tenantid/oauth2/v2.0/token Refresh endpoint: https://login.microsoftonline.com/tenantid/oauth2/v2.0/refresh Scope: QnA.Read.All  Save the information. A new OAuth registration key will be generated:\nOpen the ai-plugin.json file and replace the auth property by the following, using the key from the previous step:  \u0026#34;auth\u0026#34;: {  \u0026#34;type\u0026#34;: \u0026#34;OAuthPluginVault\u0026#34;,  \u0026#34;reference_id\u0026#34;: \u0026#34;ZTRhNDM5YjQtM2...\u0026#34; },  For CI/CD scenarios, you can also use a token like ${{OAUTH2_REGISTRATIONKEY}} and add the value in the .env file.\n To call the plugin, you need to make sure the instructions are clear for the agent so it can makes valid requests.\n6. Update prompt instructions to call the plugin Open the instruction.txt file and add the following prompt:\n- You are a declarative agent answering user question according the a QnA knowledge base: - For all questions, complement your answer by calling the \u0026#39;action_1\u0026#39; plugin with the following API body format and by replacing the {query} token by the user query transformed as search keywords and translated to English. Use no more than 3 keywords enclosed by double quotes and separated by an \u0026#39;OR\u0026#39; condition, for example \u0026#34;bing\u0026#34; OR \u0026#34;search\u0026#34; OR \u0026#34;security\u0026#34;. {  \u0026#34;requests\u0026#34;: [  {  \u0026#34;entityTypes\u0026#34;: [  \u0026#34;qna\u0026#34;  ],  \u0026#34;query\u0026#34;: {  \u0026#34;queryString\u0026#34;: \u0026#34;{query}\u0026#34;  }  }  ] } We are now ready to test our plugin!\n7. Test with some QnAs! For testing purpose, we use builtin Microsoft Search QnAs about the Bing search engine:\nWhen using QnAs, the user query match is done according to the keywords defined for each QnA. This is an exact match so make sure keywords are relevant. Also we treat all keywords in English avoding us to provide QnA keywords for each language. That is why we ask the agent to translate all search keywords in that langauge.\nAlso, to make sure our plugin is triggered, we need to turn on the develper mode in Copilot using the prompt: -developer on.\nIn Visual Studio, from Teams Toolkit, in the Lifecycle section, click on \u0026ldquo;Provision\u0026rdquo; (you must have Teams custom app side loading enabled for your account). Then open the https://www.office.com/chat?auth=2 page. You should see your agent on the right column.\nYouc can start asking questiosn based on your QnAs to trigger the plugin. You will have first to confirm the plugin action (this behavior can be changed), and then to authenticate.\nIf a keyword match one of the QnA defined in Microsoft Search, the content will be used by Copilot to build the answer:\nThe verify the plugin has been called, you can expand the developer information to get the details:\nAs you can see, the Microsoft Graph API can also be used as a plugin in a Copilot agent, enabling various possiblities. The QnA usage demonstrated here is a good strategy to enhance the grounded data and complement other sources to improve answers accuracy.\n","date":"January 16, 2025","image":null,"permalink":"/post/copilot-graph-api-qna-plugin/","title":"Use the Microsoft Graph API as a Copilot plugin for a declarative agent"},{"categories":["docusaurus","Microsoft Search"],"contents":"In this blog post, I show you how to integrate the PnP Modern Search Core components into your enterprise Docusaurus documentation sites to add search capability.\n The GitHub solution associated to this post is available here: https://github.com/microsoft-search/pnp-modern-search-core-components/tree/main/samples/docusaurus\n What is Docusaurus? Docusaurus (https://docusaurus.io/) is a static site generator created by Facebook and implemented with React. It provides an easy way for dev teams to write their documentation using Markdown syntax. I personnaly use it for all my projects (interna/external) as it contains a lot of features and is very easy to use and customize.\nIf you don\u0026rsquo;t know about this tool, I strongly encourage you to take a look.\nLimitation with search By default, Docusaurus doesn\u0026rsquo;t come with a built-in search engine for your website. It is up to you to configure one. For public websites with no authentication, it does provides a straightforward integration with Algolia. For example, this what I use to use for the public documentation of the PnP Modern Search Components and it works quite well.\nThe thing is, in a enterprise scenario, if you decided to use Docusaurus to write your internal documentation, you won\u0026rsquo;t likely expose it publicly to be indexed by a third party tool just to get a search capability.\nFor only few sites in your organization, you might say search is not that important if navigation is well managed. Fair enough. But when you have several sites made by different teams having their own structure with a lot of content, the search feature becomes essential.\nIn the next sections, I\u0026rsquo;ll detail how to use Microsoft Search to index an internal documentation site and provide a search experience directly in it using PnP search components.\nPrepare your site for Microsoft Search Configure meta-tags Having search capabilities often means having relevant filters for your content. Regarding websites, this can be achieved by leveraging HTML meta-tags.\nIn Docusaurus, meta-tags can be set directly in the docusaurus.config.ts file:\nthemeConfig: { \t... \tmetadata: [ \t{name: \u0026#39;projectName\u0026#39;, property: \u0026#34;projectName\u0026#34;, content: \u0026#39;PnP Modern Search Core - Docusaurus Demo\u0026#39;}, \t{name: \u0026#39;projectType\u0026#39;, property: \u0026#34;projectType\u0026#34;, content: \u0026#39;Public\u0026#39;}, \t{name: \u0026#39;projectTechnologies\u0026#39;, property: \u0026#34;projectTechnologies\u0026#34;, content: \u0026#39;Docusaurus,TailwindCSS,Azure DevOps,Microsoft Search\u0026#39;}, \t{name: \u0026#39;projectDescription\u0026#39;, property: \u0026#34;projectDescription\u0026#34;, content: \u0026#39;Integration demo of PnP Modern Search core components in a Docusaurus site\u0026#39;}, \t{name: \u0026#39;projectRepository\u0026#39;, property: \u0026#34;projectRepository\u0026#34;, content: \u0026#39;https://github.com/microsoft-search/pnp-modern-search-core-components\u0026#39;}, \t{name: \u0026#39;projectContact\u0026#39;, property: \u0026#34;projectContact\u0026#34;, content: \u0026#39;Franck Cornu\u0026#39;}, \t{name: \u0026#39;source\u0026#39;, property: \u0026#34;source\u0026#34;, content: \u0026#39;Documentation\u0026#39;}, \t], \t... } satisfies Preset.ThemeConfig When configured here, meta-tags are available for all the pages in your site:\nThis configuration is required to make sure all the content is tagged correctly. If needed, you can also set meta-tags at page level.\nThese meta tags will be then mapped to your search schema in Microsoft Search when creating the connector.\nDeploy your site In my context, I use an Azure App Service (Windows, NodeJS 20+) to host the documentation. Docusaurus being a static site generator, it technically doesn\u0026rsquo;t require a web server and can be deloyed to an Azure Static Web App or an Azure Blob container with the static webiste feature enabled (in this case, you can\u0026rsquo;t add authentication on top like an App Service or a Static Web Site provide with EasyAuth). When deploying a Docusaurus site to an App Service, you\u0026rsquo;ll need to write a dummy server and configure a web.config rewrite rule to serve static assets from Docusaurus build folder. In my case I use a basic Node.js Express server:\nimport express from \u0026#34;express\u0026#34;;  const app = express(); const port = process.env.port || process.env.PORT || 3333;  // \u0026#39;build\u0026#39; is the folder where the static contents are generated by Docusaurus app.use(express.static(__dirname + \u0026#39;/build\u0026#39;));  app.listen(port, () =\u0026gt; {  console.log(`Server started on Azure on port ${port}`) }); Also, for convenience, and to avoid uploading the whole node_modules, I\u0026rsquo;m using Webpack to get a standalone bundle file to be uploaded:\n... \u0026lt;rewrite\u0026gt;  \u0026lt;rules\u0026gt; \t... \t\u0026lt;!-- All other URLs are mapped to the node.js site entry point --\u0026gt; \t\u0026lt;rule name=\u0026#34;DynamicContent\u0026#34;\u0026gt; \t\u0026lt;conditions\u0026gt; \t\u0026lt;add input=\u0026#34;{REQUEST_FILENAME}\u0026#34; matchType=\u0026#34;IsFile\u0026#34; negate=\u0026#34;True\u0026#34;/\u0026gt; \t\u0026lt;/conditions\u0026gt; \t\u0026lt;action type=\u0026#34;Rewrite\u0026#34; url=\u0026#34;server.js\u0026#34;/\u0026gt; \t\u0026lt;/rule\u0026gt;  \u0026lt;/rules\u0026gt; \u0026lt;/rewrite\u0026gt; ... In the end, once deployed to the App Service, my application looks like this:\nConfigure authentication The next step is to configure autentication for your deployed web application. In an enterprise scenario, you likely want your site to be protected by Entra ID. For this part, you have basically two options:\n  Implement the authentication part directly in your Docusaurus site, for instance using MSAL.js (Kudos to my fellow coworker @Igor Bertnyk!). The issue with that approach is your site content will become inaccessible to the search crawler. The crawler being a daemon process with no user interaction and MSAL.js being purely client-side, when it will hit your page, it will just see the sign-in page and will you will end-up with only 1 page indexed as it can\u0026rsquo;t authenticate.\n  Use the \u0026quot;EasyAuth\u0026quot; feature of App Service. With this type of authentication, your content will remain accessible to the crawler, as the authentication will be performed before accessing to your site. More importantly, the Microsoft Search Graph connector Enterprise Websites  connectors (Cloud or On-Premises) support this scenario and will be able to connect to your site this way.\n  Create connector in Microsoft Search In enterprise search scenario, you must use the Enterprise Websites \u0026ldquo;On-Premise\u0026rdquo; connector as your webiste won\u0026rsquo;t be available over the public internet, and therefore accessible by the Microsoft cloud connector.\nIn this case, you need first to setup and register a machine with the Graph Connector Agent that has access to this URL (most of the time, a virtual machine inside the company internal network boundary).\nIn the Microsoft Search admin center, create a new connector, setting the information for your website. For the authentication part, use the application ID for both resource and clients ID fields and use a generated client secret from the Entra ID application used by EasyAuth:\n The website needs to be hosted in the same tenant as the Microsoft 365 one. You can\u0026rsquo;t index an internal website protected by Entra ID hosted in a different tenant\n Once the connection is authorized, on the \u0026ldquo;Content\u0026rdquo; tab, setup your search schema mapping the meta-tags you defined earlier:\nIntegrate components into your site Once the connector is ready, we can now integrate PnP Modern Search components into the website to get results.\nIn the package.json, first add the @pnp/modern-search-core dependency:\n... \u0026#34;dependencies\u0026#34;: { \t..  \u0026#34;@pnp/modern-search-core\u0026#34;: \u0026#34;1.1.1\u0026#34; \t... } Configure the components with authentication and deal with SSR Because Docusaurus uses React SSR (Server Side Rendering), you can\u0026rsquo;t import components directly using regular import { \u0026lt;...\u0026gt; } from '@pnp/modern-search-core' statements on top of your files as this will fail the build process. PnP Modern Search components are web components meant to be executed in the browser and can\u0026rsquo;t be packaged for server side rendering (for instance the \u0026lsquo;window\u0026rsquo; object is not accesssible from there). As a workaround, dependencies are loaded asynchronously, when the actual page is rendered in the browser (see https://docusaurus.io/docs/advanced/ssg for more info about SSR workarounds):\nWe use the swizlling technique to override the root component theme/Root.tsx (rendered once in this application), to load the components asynchronously. This way, components can be used on all pages:\nimport { useEffect} from \u0026#34;react\u0026#34;;  export default function Root({children}) {   useEffect(() =\u0026gt; {   const initialize = async () =\u0026gt; {   await import(\u0026#39;@pnp/modern-search-core/dist/es6/exports/define/pnp-search-results\u0026#39;);  await import(\u0026#39;@pnp/modern-search-core/dist/es6/exports/define/pnp-search-input\u0026#39;);  await import(\u0026#39;@pnp/modern-search-core/dist/es6/exports/define/pnp-search-filters\u0026#39;);  await import(\u0026#39;@pnp/modern-search-core/dist/es6/exports/define/pnp-search-verticals\u0026#39;);   const { LoginType, Providers, SimpleProvider, ProviderState, Msal2Provider,TemplateHelper, registerMgtLoginComponent } = await import(\u0026#39;@pnp/modern-search-core\u0026#39;);   const login = async () =\u0026gt; {   // Easy auth scenario  try {  await fetch(\u0026#39;/.auth/login\u0026#39;);  } catch (error) {  console.error(\u0026#39;Error while login\u0026#39;, error);  return null;  }  }   const logout = async () =\u0026gt; {   // Easy auth scenario  try {  await fetch(\u0026#39;/.auth/logout\u0026#39;);   } catch (error) {  console.error(\u0026#39;Error while logout\u0026#39;, error);  return null;  }  }   const getAuthInfo = async () =\u0026gt; {   // Easy auth scenario  try {  const response = await fetch(\u0026#39;/.auth/me\u0026#39;);  const payload = await response.json();  return payload \u0026amp;\u0026amp; payload[0];  } catch (error) {  console.error(\u0026#39;Error while getting access token\u0026#39;, error);  return null;  }  };   const refreshAccessToken = async () =\u0026gt; {   // Easy auth scenario  try {  const response = await fetch(\u0026#39;/.auth/refresh\u0026#39;);  const payload = await response.json();  return payload \u0026amp;\u0026amp; payload[0];  } catch (error) {  console.error(\u0026#39;Error while getting access token\u0026#39;, error);  return null;  }  };   const getAccessToken = async () =\u0026gt; {   const authInfo = await getAuthInfo();  if (new Date(authInfo.expires_on) \u0026gt;= new Date()){  const refreshToken = await refreshAccessToken();   return refreshToken;  }  return authInfo?.access_token;  };   const authInfo = await getAuthInfo();   if (authInfo) {  const simpleProvider = new SimpleProvider(getAccessToken, login, logout);   simpleProvider.getActiveAccount = () =\u0026gt; {  return {  id: authInfo.user_id  }  }   Providers.globalProvider = simpleProvider;  Providers.globalProvider.setState(ProviderState.SignedIn)   } else {   Providers.globalProvider = new Msal2Provider({  clientId: process.env.ENV_MSSearchAppClientId,  authority: `https://login.microsoftonline.com/${process.env.ENV_MSSearchAppTenantId}`,  domainHint: process.env.ENV_MSSearchAppDomain,  redirectUri: `${process.env.ENV_SiteUrl}`,  loginType: LoginType.Popup  });  }   // To avoid conflicts with MDX  TemplateHelper.setBindingSyntax(\u0026#39;[[\u0026#39;, \u0026#39;]]\u0026#39;);   registerMgtLoginComponent();  }   initialize();  });   return children; } For authentication, we create a dummy Microsoft Graph Toolkit provider, getting access token directly from EasyAuth builtin endpoints (/.auth/me|refresh|login|logout).\nThe next step is now to provide a basic search experience, with a search box always accessible on top of pages and a dedicated search page to browse results.\nAdding the searchbox Docusaurus allows you to override the default search bar rendering by providing your own component. Again, we use Docusaurus swizzling technique by creating the theme/SearchBar.tsx file and adding the PnP Search box components. We also add the Microsoft Graph Tooolkit login component to indicate if the user is logged in or not (with the possibility to logout if needed):\nimport { wrapWc } from \u0026#39;wc-react\u0026#39;; import { MgtLogin, SearchInputComponent } from \u0026#39;@pnp/modern-search-core\u0026#39;; import { PageOpenBehavior, QueryPathBehavior } from \u0026#39;@pnp/modern-search-core/dist/es6/helpers/UrlHelper\u0026#39;; import React from \u0026#34;react\u0026#34;; import BrowserOnly from \u0026#39;@docusaurus/BrowserOnly\u0026#39;;  const SearchInputWebComponent = wrapWc\u0026lt;Partial\u0026lt;SearchInputComponent\u0026gt;\u0026gt;(\u0026#39;pnp-search-input\u0026#39;); const MgtLoginWebComponent = wrapWc\u0026lt;Partial\u0026lt;MgtLogin\u0026gt;\u0026gt;(\u0026#39;mgt-login\u0026#39;);  export default function SearchBar() {   return \u0026lt;div className=\u0026#39;flex items-center space-x-3\u0026#39;\u0026gt;  \u0026lt;BrowserOnly\u0026gt;  {() =\u0026gt; {  return \u0026lt;\u0026gt;  \u0026lt;SearchInputWebComponent  id=\u0026#39;f566fb98-4010-485b-a44b-b53c16852cd6\u0026#39;  inputPlaceholder=\u0026#39;Enter a keyword...\u0026#39;  searchInNewPage={true}  pageUrl={`${new URL(`${process.env.ENV_SiteUrl}/search`)}`}  queryPathBehavior={QueryPathBehavior.QueryParameter}  queryStringParameter=\u0026#39;k\u0026#39;  defaultQueryStringParameter=\u0026#39;k\u0026#39;  openBehavior={PageOpenBehavior.Self}  /\u0026gt;   \u0026lt;MgtLoginWebComponent loginView=\u0026#39;avatar\u0026#39;\u0026gt;\u0026lt;/MgtLoginWebComponent\u0026gt;  \u0026lt;/\u0026gt;;  }}  \u0026lt;/BrowserOnly\u0026gt;  \u0026lt;/div\u0026gt;; } Few things to notice here:\n We consume PnP Modern Search components as React components there thanks to the wc-react utility library. This allows to easily set component properties:  const SearchInputWebComponent = wrapWc\u0026lt;Partial\u0026lt;SearchInputComponent\u0026gt;\u0026gt;(\u0026#39;pnp-search-input\u0026#39;);   Because Docusaurus uses React SSR (Server Side Rendering), it is important to enclose the component with the \u0026lt;BrowserOnly\u0026gt; wrapper. This basically excludes the components from server compilation.\n  We use Webpack EnvironmentPlugin to dynamically set the search page URL using environment variables at bundling time.\n  Creating the search page Last step of the process, we create the dedicated search page search.tsx using again, React components enclosed with \u0026lt;BrowserOnly\u0026gt; wrapper:\nimport Layout from \u0026#34;@theme/Layout\u0026#34;; import { wrapWc } from \u0026#34;wc-react\u0026#34;; import { SearchFiltersComponent, SearchInputComponent, SearchResultsComponent } from \u0026#34;@pnp/modern-search-core\u0026#34;; import { BuiltinFilterTemplates } from \u0026#34;@pnp/modern-search-core/dist/es6/models/common/BuiltinTemplate\u0026#34;; import { FilterConditionOperator } from \u0026#34;@pnp/modern-search-core/dist/es6/models/common/IDataFilter\u0026#34;; import { FilterSortDirection, FilterSortType } from \u0026#34;@pnp/modern-search-core/dist/es6/models/common/IDataFilterConfiguration\u0026#34;; import { EntityType } from \u0026#34;@pnp/modern-search-core/dist/es6/models/search/IMicrosoftSearchRequest\u0026#34;; import BrowserOnly from \u0026#34;@docusaurus/BrowserOnly\u0026#34;;  const SearchInputWebComponent = wrapWc\u0026lt;Partial\u0026lt;SearchInputComponent\u0026gt;\u0026gt;(\u0026#39;pnp-search-input\u0026#39;); const SearchFiltersWebComponent = wrapWc\u0026lt;Partial\u0026lt;SearchFiltersComponent\u0026gt;\u0026gt;(\u0026#39;pnp-search-filters\u0026#39;); const SearchResultsWebComponent = wrapWc\u0026lt;Partial\u0026lt;SearchResultsComponent\u0026gt;\u0026gt;(\u0026#39;pnp-search-results\u0026#39;);  export default function Searchpage(): JSX.Element {   return \u0026lt;Layout\u0026gt;  \u0026lt;main className=\u0026#34;container container--fluid margin-vert--lg\u0026#34;\u0026gt;  \u0026lt;BrowserOnly\u0026gt;  {() =\u0026gt; {  return \u0026lt;\u0026gt;  \u0026lt;SearchInputWebComponent  id=\u0026#39;edfdea93-23c9-4ba6-94d9-a848a1384104\u0026#39;  inputPlaceholder=\u0026#39;Enter a keyword...\u0026#39;  defaultQueryStringParameter={\u0026#34;k\u0026#34;}  \u0026gt;\u0026lt;/SearchInputWebComponent\u0026gt;   \u0026lt;SearchFiltersWebComponent  id=\u0026#34;4f5ad5cd-8626-40ab-9c89-466a64f57a8e\u0026#34;  filterConfiguration={[  {  displayName: \u0026#34;Type\u0026#34;,  filterName:\u0026#34;filetype\u0026#34;,  template: BuiltinFilterTemplates.CheckBox,  isMulti:false, maxBuckets:50,  operator: FilterConditionOperator.OR,  showCount: false,  sortBy: FilterSortType.ByCount,  sortDirection: FilterSortDirection.Ascending,  sortIdx:1  },  {  displayName: \u0026#34;Created\u0026#34;,  filterName:\u0026#34;createddatetime\u0026#34;,  template: BuiltinFilterTemplates.CheckBox,  isMulti:false, maxBuckets:50,  operator: FilterConditionOperator.OR,  showCount: false,  sortBy: FilterSortType.ByCount,  sortDirection: FilterSortDirection.Ascending,  sortIdx:1  }]}  searchResultsComponentIds={[\u0026#34;97c0a5dd-653b-4b41-8ced-7ed0feb7da88\u0026#34;,\u0026#34;68f37193-8a2e-4c09-8337-1c3db978ccb8\u0026#34;]}  \u0026gt;\u0026lt;/SearchFiltersWebComponent\u0026gt;   \u0026lt;SearchResultsWebComponent  id=\u0026#34;68f37193-8a2e-4c09-8337-1c3db978ccb8\u0026#34;  defaultQueryText=\u0026#34;*\u0026#34;  queryTemplate=\u0026#34;{searchTerms}\u0026#34;  selectedFields={[\u0026#34;title\u0026#34;,\u0026#34;weburl\u0026#34;,\u0026#34;filetype\u0026#34;,\u0026#34;source\u0026#34;,\u0026#34;keywords\u0026#34;,\u0026#34;url\u0026#34;,\u0026#34;iconUrl\u0026#34;,\u0026#34;createddatetime\u0026#34;]}  searchInputComponentId=\u0026#34;edfdea93-23c9-4ba6-94d9-a848a1384104\u0026#34;  searchFiltersComponentId=\u0026#34;4f5ad5cd-8626-40ab-9c89-466a64f57a8e\u0026#34;  connectionIds={[`/external/connections/${process.env.ENV_MSSearchConnectionId}`]}  entityTypes={[EntityType.ExternalItem]}  showDebugData={true}  pageSize={10}  \u0026gt;   \u0026lt;template data-type=\u0026#34;items\u0026#34;\u0026gt;  \u0026lt;div data-for=\u0026#39;item in items\u0026#39;\u0026gt;  \u0026lt;div className=\u0026#34;mt-4 mb-4 ml-1 mr-1 grid gap-2 grid-cols-searchResult\u0026#34;\u0026gt;  \u0026lt;div className=\u0026#34;h-7 w-7 text-textColor m-1\u0026#34;\u0026gt;  \u0026lt;img src=\u0026#34;[[ item.iconurl ]]\u0026#34;/\u0026gt;  \u0026lt;/div\u0026gt;  \u0026lt;div className=\u0026#34;dark:text-textColorDark\u0026#34;\u0026gt;  \u0026lt;div className=\u0026#34;font-semibold mt-1 mb-1 ml-0 mr-0 \u0026#34;\u0026gt;  \u0026lt;a href=\u0026#34;[[item.url]]\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;[[ item.title]]\u0026lt;/a\u0026gt;  \u0026lt;/div\u0026gt;  \u0026lt;div className=\u0026#34;mt-1 mb-1 ml-0 mr-0 line-clamp-4\u0026#34; data-html\u0026gt;  [[item.summary]]  \u0026lt;/div\u0026gt;  \u0026lt;/div\u0026gt;  \u0026lt;/div\u0026gt;  \u0026lt;/div\u0026gt;  \u0026lt;/template\u0026gt;   \u0026lt;/SearchResultsWebComponent\u0026gt;  \u0026lt;/\u0026gt;;   }}  \u0026lt;/BrowserOnly\u0026gt;  \u0026lt;/main\u0026gt;  \u0026lt;/Layout\u0026gt; } To consume results from the Microsoft Graph Connector defined ealrier, we specify the connection ID in the SearchResultsWebComponent component like this:\nconnectionIds={[`/external/connections/${process.env.ENV_MSSearchConnectionId}`]} The rest of the configuration is done according components documentation and properties.\nEt voilà! Now when the website is accessed, users will be logged automatically via EasyAuth and will be able to search for website content site using components and Microsoft Search:\nYou can see the full working example here: https://github.com/microsoft-search/pnp-modern-search-core-components/tree/main/samples/docusaurus\n","date":"January 3, 2025","image":null,"permalink":"/post/integrate-components-with-docusaurus/","title":"Leverage PnP Modern Search Core components and Microsoft Search in your Docusaurus documentation"},{"categories":["copilot","sharepoint"],"contents":"In this blog post, I wanted to share with you a use case I came across regarding my recent experience with Copilot Studio.\n DISCLAIMER  I\u0026rsquo;m not an \u0026ldquo;AI expert\u0026rdquo; by any means, just a developer who consumes AI services which is not quite the same :D.\n  IMPORTANT UPDATE (23/08/2024)  I had the confirmation from Microsoft that custom Copilots built through Copilot Studio do not leverage the user or tenants semantic index. It uses a combination of regular lexical search and internal summarization steps passed to LLM to produce answers. Therefore, it explains the behavior I\u0026rsquo;ve seen in my example. This is not clearly documented though. The best hint can be found here.\n Context I recently started a proof of concept to evaluate Copilot capabilities using Copilot Studio. The business goal was basically to help people working on an specific domain to navigate through SharePoint documentation content using a dedicated assistant answering questions about it.\nAll this information was stored as modern pages in multiple SharePoint sites using a custom information architecture. Several metadata were used for classification and search purposes (most of them being managed metadata defined in the taxonomy term store).\nThe main issue was, even with regular search experience with filters, users were still struggling to find the relevant information due to the amount of content, especially how things were related together through metadata. It was a perfect use case for a Copilot project.\nCosting and licences considerations for Copilot Studio AI is cool but it is quite expensive and costs have to be justified and planned before implementing anything. As of today (August 2024), to build Copilots within Copilot Studio, you have two options:\n  Copilot for Microsoft 365: this licence allows you to extend existing copilots by writing plugins, for instance making available specific data sources or actions.\nThis plugin can be then published whithin the organisation for other users, requiring them to have a Copilot for Microsoft 365 licence as well. With this type of licence you can\u0026rsquo;t build (actually share) a standalone Copilot (i.e a bot acessible by other users without Copilot licence).\nFor my use case, giving a full Copilot for Microsoft 365 licence for hundred of users was not an option. Despite this licence comes with unlimited messages quota and full integrations (like Word, Excel, etc.), most of the time, only few features are really useful. In my case I wanted an assistant accessible on-demand by anyone inside the company.\n  Copilot Studio licence(s): For Copilot Studio, you need two types of licences:\n  a tenant licence coming with a \u0026ldquo;capacity pack\u0026rdquo; of 25 000 messages per month. This capacity is shared by all the standalone Copilots you create. According to Microsoft, a \u0026ldquo;billed message is a request or message sent to the copilot triggering an action and/or response\u0026rdquo;. A Regular answer (Non-generative AI) costs 1 message but a generative AI (Gen AI) answer over your data costs 2 messages. It means you could create a Copilot without using any AI features. Talking about AI, the key element in Copilot Studio enabling the AI capability is called generative answers. It is a special action available in the editor allowing you to use LLM over your data:\n Generative answer  It is not clear what model Microsoft use to do this, but it appears to be GPT 3.5 Turbo as the documentation states corpus upon which the model was trained doesn\u0026rsquo;t include data created after 2021. I don\u0026rsquo;t see Microsoft using a more expensive GPT4 model here.\n   a user licence (Free) allowing an user to author and publish Copilots in the Copilot Studio interface.\n  The main interest of the Copilot Studio licence(s) is that users of your Copilots don\u0026rsquo;t need to have a Copilot for Microsoft 365 licence to consume them. Also you can publish on multiple channels (Teams, web, etc.). Moreover, having a quota system reduces the risk to pay for unused features and allows you to be more in control regarding costs. When you see the usage growing, you can simply add more quotas by buying an other capacity pack, going steps by steps and measuring adoption along the way. It is also important to notice unused messages inside quotas do not carry over month to month. If you don\u0026rsquo;t use your quotas, well, too bad for you. That\u0026rsquo;s why monitoring is an important part of the process.\n  IMPORTANT: It seems having a Copilot Studio tenant licence will allow users with Copilot for Microsoft 365 licences to author, publish and share a standalone Copilots in Teams only (other channels will be blocked). I didn\u0026rsquo;t find any information about this but this is the behavior I noticed in my tenant.\nBuilding a copilot using Copilot Studio Let\u0026rsquo;s take a fictional example of a \u0026ldquo;Marvel\u0026rdquo; copilot, answering questions about Marvel\u0026rsquo;s universe (characters, locations, etc.), the information being stored as modern pages in multiple SharePoint sites corresponding to each entity (i.e. character pages stored in a \u0026ldquo;Character\u0026rdquo; site, etc. ).\n Note that I\u0026rsquo;ve been able to reproduce the same exact behavior with different type of content so it means you could reproduce it on your own tenant as well.\n The experience of building a copilot with Copilot Studio is quite straightforward. I was able to select the SharePoint sites containing all the information to retrieve. I started with a very basic behavior with only one topic configured using the generative answer action and setting multiple SharePoint sites as knowledge sources (without using Copilot general knowledge). For the initial prompt, I started with this simple prompt:\nYou are a personal assistant for company employees. You will answer questions about documentation from the SharePoint internal documentation. Your answers will be precise and complete.  First test: questions about information present in the page content I started with simple questions like asking for a custom specific information, like the full name of a character, an information present both in the page content and in a dedicated metadata (in this case a text field):\nAs an example for this particular question: \u0026ldquo;What is the full name of \u0026ldquo;Iron Man\u0026rdquo;?\u0026rdquo;, I\u0026rsquo;ve tested two configurations:\n  Configuration #1: Targeting all SharePoint sites (i.e all entities like characters, locations, etc.) in a single generative answer: no results, Copilot saying there was not enough information in data sources to answer this question even though this information clearly appears both in page text and metadata.\n  Configuration #2: Creating a topic dedicated to characters and targeting only the \u0026ldquo;Characters\u0026rdquo; SharePoint site. This time the Copilot gave the right answer (but not all the time).\n  The first lesson learned from that test is narrowing to specific sources seems to improve a lot the quality of responses. That is not really surprising. Behind the scenes, despite it is not officialy documented, Copilot only processes a subset of the results from the semantic index to generate its answer (probably around first 10 results for performances and tokens restrictions reasons). More sources you have, more results you\u0026rsquo;ll get and potentially less relevant results to build the answer from anyway.\nSecond test: questions about information present only in page metadata In my second test, I wanted to evaluate how Copilot was able to leverage pages metadata. For this test, I focused my questions on characters having their full names only specified as metadata (i.e. as a SharePoint column) but not in the content of the page itself. As a result, the Copilot gave either no answer or completly wrong answer EVERY. SINGLE. TIME.. I rapidly figured out that my custom information architecture wasn\u0026rsquo;t taken into account at all\u0026hellip;\nFor a tool supposed to save you time, it was actually doing the opposite: the need to countercheck the information resulting to a lack of trust and therefore adoption :/.\nWhy custom metadata are not considered? Knowing a little bit about Microsoft Search and Copilot, I think I figured out why my custom metadata were not taken into account.\n\u0026ndash;SUPPOSITION MODE ON\u0026ndash;\nCopilot is based on Microsoft Search and its semantic index to get data from various sources and generate answers from them. Behind the scenes, the semantic index generates embeddings from content (i.e. creating vectors from pieces of content). These embeddings are then compared together to find similarities according to a query and then returned to LLM to build an answer from these extracts. As a Copilot customer, you have no control over this process nor the models used to generate embeddings from your content.\nOn the Microsoft Search side now, when you define a data source, you must define its search schema and specify a \u0026ldquo;content\u0026rdquo; property. The content property represents the primary content of your item (ex: the content of a page or a document). SharePoint is no different from other sources here regarding search and also has a content property defined somewhere in its schema. In the case of a SharePoint modern page, the CanvasContent1 property is likely used as content property to generate embeddings in the semantic index. This is where the raw content of the page is (i.e. text in the \u0026ldquo;Text\u0026rdquo; Web Part but also other components).\nAssuming this, then how Microsoft could possibly know what metadata from your custom schema are important and what are the ones who are not when generating embeddings? Short answer: they can\u0026rsquo;t as their meaning is unique to you and your context so my guess is they are simply ignored in the process. In that sense, that is understandable because this could lead to incorrect information retrieved and generate noise for answers. Fair enough.\n\u0026ndash;SUPPOSITION MODE OFF\u0026ndash;\n Important update 23/08/2024: I had the confirmation from Microsoft that custom Copilots built through Copilot Studio do not leverage the user or tenants semantic index. However, the supposition made above is still valid for Copilot for Microsoft 365 scenarios as it uses semantic index in this case.\n Tricking the Copilot index Because I had no control over the index, I tried multiple solutions to \u0026ldquo;trick\u0026rdquo; the index and make the metadata information available to generate the answers. I tested few strategies:\n Metadata as text (original page)  In this approach the goal was simply to explicity add the metadata as text in the original page at the very bottom:\nUnlike the first test I did and even though the information was present in the page, the Copilot was not able to give the right answer.\nAgain, I think this is because of how embeddings are generated and how the content is structured in the page itself. It means even though the information is present in the page, it does not guarantee it will be used for answers. Well\u0026hellip;\nMetadata in a new page.  In that second approach, I completely created a new page using the same title as the original one and added the metadata as text in the content:\nThis time, asking the place or origin, the Copilot gave me the right answer!\nIt seems creating a new page using the same title but only including the metadata as text gives the correct output. This way this page comes high in the results (because of the title) and can be used for answers. Despite this approach seems to work, that is definitely not a long term satisfying option\u0026hellip;\nHere is a summary of strategies I tested and the Copilot outcomes:\n   Approach Description Copilot outcomes     #1 Metadata only The information is only contained in the page metadata as SharePoint columns. Wrong or no answer at all.   #2 Metadata as text (original page) Metadata values are explicilty written in the page in a text Web Part at the bottom of the page. Wrong or no answer at all.   #3 Metadata as text (new page) Metadata values are explicilty written in the page in a text Web Part in a new page with same title. Correct answer.    Conclusion For years, Microsoft told us to use metadata to classify the information to improve search and discoverability. Despite this is still a good practice for regular search experiences, with the new hype of GenAI, it seems these metadata are totally forgotten in Copilot experiences and custom information architectures are now becoming pretty much useless for such scenarios\u0026hellip;\nIn my case, that was a big disapointement because, with Microsoft Search (Graph Connector in the context of Copilot if you prefer) many other sources (not only SharePoint) can contain custom schemas with a lot of valuable information that could be used to generate answers. As it is now, I can\u0026rsquo;t use Copilot Studio for this type of requirements as the outcomes are not reliable.\nWhat about a custom Copilot with Teams Toolkit\u0026hellip;?\nEven with a custom Copilot scenario with Teams Toolkit and Teams AI library, there is no way to do RAG with the Microsoft Search semantic index programmaticaly as there is no (official) API.\nWhy not using Azure Search instead\u0026hellip;?\nIndexing SharePoint content into a other system like Azure Search with more flexibility could theorically work better but, what would be the point to re-index content into an other system, at additional costs, while this data is already indexed into a service, Microsoft 365, I already pay for? An absolute nonsense. Anyway Azure Search SharePoint connector does not support .aspx pages and do not even mention the complexity of managing content permissions.\nOk but what about a custom Copilot plugin then..?\nPlugins are good to fetch live data based on a prompt and use returned data in the generated answer. Technically speaking, to get a specific metadata value to be considered in the answer, that could be a solution. However, it just requires few steps\u0026hellip;:\n Interpret the user query and detect every possible metadata he could ask for to trigger the plugin. Identify uniquely the content that I need to get the metadata from based on that query, like the corresponding SharePoint pages. These information should theorically come from the semantic index in the first place. In the plugin, call the Microsoft Graph API for all these pages based on the URL and get the metadata values.  What a complex implementation with an high chance of failure for something that could have been simply resolved by retrieving metadata value along the content in the first place\u0026hellip;\nI really hope Microsoft will do something about it because the thousand of companies who invested in a custom SharePoint information architecture won\u0026rsquo;t certainly appreciate this limitation. I already see some solutions that could help:\n In Copilot Studio, have a way to customize the underyling KQL Query and/or select the search managed properties to retrieve in the data source results and considered to generate answers. For custom scenarions, provide an API (like a grounding API) in Graph to consume the semantic index and get content extracts according to a KQL query. This API should also be able to select fields (i.e managed properties) to be retrieved so we could add them to the prompt doing our own RAG, for instance in a custom bot. This API could be tied to the Copilot Studio licences and quotas, to make sure you had to go through Copilot offer and not bypass the licensing restrictions.  Bonus: other Copilot Studio limitations I found Other than the metadata issues I faced, I also encoutered less impacful limitations in Copilot Studio like:\n You don\u0026rsquo;t have access to the underlying query made to the search sematic index. You can\u0026rsquo;t write your own KQL Query, for example to filter on specific well known search properties according to a topic to narrow retreived data. You can\u0026rsquo;t reset a conversation using a Copilot in Teams. Because the Copilot bot keeps conversation history, it can be stuck in wrong context giving several wrong answers in a row. Sometimes, starting over could be nice when your Copilot starts to malfunction. Sometimes the language of copilot answers can change unexpectedly, even if the specified language is set to \u0026ldquo;English\u0026rdquo; in parameters and prompt instructions specify English only. For instance, asking a question about \u0026ldquo;Bennet du Paris\u0026rdquo; could return answers in French sometimes. You can\u0026rsquo;t choose your LLM model. Currently it uses GPT 3.5 turbo (probably). To select your own, you can combine Copilot studio and Azure Open Azure AI but it requires additional costs and overlaps with Copilot Studio costs and value proposition. It is better to go with a custom copilot in this case. Not sure if this one is a bug but Copilot returns sometimes content from other sources than the ones specified in the action (ex: returning Confluence results even though this source hasn\u0026rsquo;t been specified anywhere in the configuration).  Cheers!\n","date":"August 1, 2024","image":null,"permalink":"/post/copilot-studio-and-metata/","title":"Hey Copilot, what about my metadata?"},{"categories":["spfx"],"contents":"    The GitHub solution showcasing the integration is available here: https://github.com/FranckyC/spfx-tailwind-sample/tree/develop    To handle CSS customizations, the SharePoint framework comes with a prebuilt mechanism in its toolchain. Basically, all SASS or CSS stylesheets can be treated as TypeScript modules (ex: mycomponent.module.scss). This way, you can directly reference classes in your components using a styles variable imported from the generated .ts module:\nimport styles from \u0026#39;./todoList.module.scss\u0026#39;; // ...  export default class TodoList extends React.Component\u0026lt;ITodoListProps, void\u0026gt; {  public render(): React.ReactElement\u0026lt;ITodoListProps\u0026gt; {  return (  \u0026lt;div className={styles.todoList}\u0026gt;  \u0026lt;div className={styles.text}\u0026gt;Hello world\u0026lt;/div\u0026gt;  \u0026lt;/div\u0026gt;  );  } } Behind the scenes, SPFx generates unique names for CSS classes to avoid name conflitcs with other components on the page and also handle CSS vendors specifities (ex: IE10/11, Safari, etc) for you.\nSo what\u0026rsquo;s the problem?\nFor simple solutions wih few components and stylesheets, this mechanism is easy to use and works well. However, for more complex solutions including many components and sub components, this approach has some limits.\nAs an developer, writing CSS is not my primary skill and not really a passion (and I\u0026rsquo;m probably not the only one there). Often I get overwhelmed by SCSS stylesheets sprawling in my project structure as long as my solution is evolving. I often need to reorganize, delete or update parts of SCSS stylesheets. I also don\u0026rsquo;t talk about hours I\u0026rsquo;ve spent fixing dummy CSS issues\u0026hellip;\nWhen writing CSS, you always have to think about naming conventions, containers hierachy, styles reusability, shared variables, etc. According to your components structure and complexity (ex: if your components needs to support theme), this is a time consuming and a cumbersome process as there is no easy way to keep in sync CSS classes usages and declarations in the code. Often, some CSS code parts aren\u0026rsquo;t actually used in my solution due to these multiple changes increasing bundle size and I always feel my styles could have been organized better.\n As an example, I\u0026rsquo;m pretty sure many CSS classes declared in the PnP Modern Search are not actually used in the code! Anyway..\n But what if you could speed up the CSS writing process, or better, completely getting rid of these numerous stylesheets and avoid writing a single line of CSS code?\nA dream for every developer no ?\nWell let\u0026rsquo;s talk about TailwindCSS.\nWhat is TailwindCSS? From the offical webiste, TailwindCSS is a utility-first CSS framework packed with predefined classes that can be composed to build any design, directly in your markup. It works only at build time.\nHow it works?\nDuring the build time, the tool scans all files defined according to your configuration (ex: all .ts, .tsx files specified in the tailwind.config.js) looking for well-known classes from the library set and generates the corresponding CSS classes. The stylesheet can then be imported into your components and classes used directly.\nWhat are the benefits?\nAt a glance:\n No more stylesheets to manage or CSS to write. You only use well known, predefined CSS classes from the library set. You don\u0026rsquo;t have to worry how it is done behind the scenes in CSS. You ship only the CSS you actually use in your code. No more dead CSS bloating your bundle. Numerous CSS utility classes to build awesome UI in minutes for non CSS specialists (like me). Many examples available from the community (paid or free depending the complexity). Heavily customizable and enough flexibility to cover edge cases. Can be integrated with any framework. Super easy light/dark mode support.  Integration with the SharePoint Framework When building the PnP Modern Search Core components and also their SPFx WebParts integration, I decided to give a try to TailwindCSS. In my company, it was already a well known library used by maby internal projects.\n I have to say, I probably never go back to traditional CSS/SASS stylesheets anymore. It saved me so much time and allowed me to create such better experiences than can\u0026rsquo;t even think doing the same writing my own CSS\n Setup TailwindCSS in your SPFx project First of all, if you work with Visual Studio Code, you can install the Tailwind CSS IntelliSense extension. Then:\n Add the TailwindCSS package in your package.json file under the devDependencies (remember, it works only at build time):  {  \u0026#34;devDependencies\u0026#34;: {  ...  \u0026#34;tailwindcss\u0026#34;: \u0026#34;3.2.4\u0026#34;,  \u0026#34;@tailwindcss/forms\u0026#34;: \u0026#34;0.5.3\u0026#34;,  } } At the root of your solution, create a tailwind.config.js file, with this default configuration:   const defaultTheme = require(\u0026#39;tailwindcss/defaultTheme\u0026#39;);  module.exports = {  mode: \u0026#39;jit\u0026#39;, // allow to update CSS classes automatically when a file is updated (watch mode). See below  content: {  files: [  \u0026#39;./src/**/*.{html,ts,tsx}\u0026#39;, // scan for these files in the solution  ]  },  corePlugins: {  preflight: false, // to avoid conflict with base SPFx styles otherwise (ex: buttons background-color)  },  darkMode: \u0026#39;class\u0026#39;,  theme: {  extend: {  fontFamily: {  sans: [\u0026#39;var(--myWebPart-fontPrimary)\u0026#39;,\u0026#39;Roboto\u0026#39;, ...defaultTheme.fontFamily.sans]  },  colors: {   /* light/dark is controlled by the theme values at WebPart level */  primary: \u0026#34;var(--myWebPart-primary, #7C4DFF)\u0026#34;,  background: \u0026#34;var(--myWebPart-background, #F3F5F6)\u0026#34;,  link: \u0026#34;var(--myWebPart-link, #1E252B)\u0026#34;,  linkHover: \u0026#34;var(--myWebPart-linkHover, #1E252B)\u0026#34;,  bodyText: \u0026#34;var(--myWebPart-bodyText, #1E252B)\u0026#34;  }  }  },  plugins: [  require(\u0026#39;@tailwindcss/forms\u0026#39;), // to be able to style inputs  ], }; This file contains basic TailwindCSS customizations for a SPFx project (CSS variables, plugins, etc.).\nCreate a styles folder in your project structure (ex: /src/styles) and create the two following files in it:   tailwind.css: this file is used as an entry point for TailwindCSS to generate your styles. It defines all the base classes you want to support.  @tailwind base; @tailwind components; @tailwind utilities;  postcss.config.js: this file is used by PostCSS to define what CSS transformations need to be done through plugins (like tailwindcss but also handling the vendors prefix support using autoprefixer plugin). This is the preferred way to run TailwindCSS along other build tools.  module.exports = {  plugins: {  tailwindcss: {},  autoprefixer: {},  } }  You can use any other PostCSS plugins you want here.\n Update your package.json to use following scripts. Also add the package npm-run-all to be able to run multiple scripts in parallel:  \u0026#34;scripts\u0026#34;: {  \u0026#34;build\u0026#34;: \u0026#34;gulp bundle\u0026#34;,  \u0026#34;build:ship\u0026#34;: \u0026#34;gulp bundle --ship\u0026#34;,  \u0026#34;clean\u0026#34;: \u0026#34;gulp clean\u0026#34;,  \u0026#34;test\u0026#34;: \u0026#34;gulp test\u0026#34;,  \u0026#34;bundle\u0026#34;: \u0026#34;npm-run-all taildwindcss build\u0026#34;,  \u0026#34;bundle:ship\u0026#34;: \u0026#34;npm-run-all taildwindcss build:ship\u0026#34;,  \u0026#34;webpack:serve\u0026#34;: \u0026#34;fast-serve\u0026#34;,  \u0026#34;serve\u0026#34;: \u0026#34;npm-run-all -p tailwindcss:watch webpack:serve\u0026#34;,  \u0026#34;taildwindcss\u0026#34;: \u0026#34;tailwindcss -i ./src/styles/tailwind.css -o ./src/styles/dist/tailwind.css --minify --postcss ./src/styles/postcss.config.js\u0026#34;,  \u0026#34;tailwindcss:watch\u0026#34;: \u0026#34;tailwindcss -i ./src/styles/tailwind.css -o ./src/styles/dist/tailwind.css --watch --minify --postcss ./src/styles/postcss.config.js\u0026#34; } {  \u0026#34;devDependencies\u0026#34;: {  ...  \u0026#34;npm-run-all\u0026#34;: \u0026#34;^4.1.5\u0026#34;  } }  I use SPFx Fast Serve tool here for convenience\n Some explanation here:\n \u0026quot;taildwindcss\u0026quot;: \u0026quot;tailwindcss -i ./src/styles/tailwind.css -o ./src/styles/dist/tailwind.css --minify --postcss ./src/styles/postcss.config.js\u0026quot;: generate the CSS stylesheet from the tailwindcss.css file using PostCSS and output the result in a /dist folder. This is this file you need to include in your code. \u0026quot;tailwindcss:watch\u0026quot;: \u0026quot;tailwindcss -i ./src/styles/tailwind.css -o ./src/styles/dist/tailwind.css --watch --minify --postcss ./src/styles/postcss.config.js\u0026quot;: same as above but in watch mode using the Just-in-Time feature. CSS classes will regenerate when your code is updated. \u0026quot;webpack:serve\u0026quot;: \u0026quot;fast-serve\u0026quot;: just a wrapper script for fast serve. \u0026quot;serve\u0026quot;: \u0026quot;npm-run-all -p tailwindcss:watch webpack:serve\u0026quot;: run the SPFx WebPart build and also TailwindCSS in watch mode using its \u0026lsquo;Just-in-time\u0026rsquo; feature. To get it work with TailwindCSS, the flag mode: 'jit' needs to be set in the tailwind.config.js configuration file. code. \u0026quot;bundle\u0026quot;: \u0026quot;npm-run-all taildwindcss build\u0026quot;: bundle the SPFx solution including TailwindCSS compilation first.  Import the stylesheet in your component and use TailwindCSS classes:  import \u0026#39;../../styles/dist/tailwind.css\u0026#39;; ...  return (  \u0026lt;section className=\u0026#39;overflow-hidden p-1 text-bodyText bg-background font-sans\u0026#39;\u0026gt;  \u0026lt;div className=\u0026#34;text-center\u0026#34;\u0026gt;  \u0026lt;img alt=\u0026#34;\u0026#34; src={isDarkTheme ? require(\u0026#39;../assets/welcome-dark.png\u0026#39;) : require(\u0026#39;../assets/welcome-light.png\u0026#39;)} className={\u0026#34;w-full max-w-[420px]\u0026#34;} /\u0026gt;  \u0026lt;h2\u0026gt;Well done, {escape(userDisplayName)}!\u0026lt;/h2\u0026gt;  \u0026lt;div\u0026gt;{environmentMessage}\u0026lt;/div\u0026gt;  \u0026lt;div\u0026gt;Web part property value: \u0026lt;strong\u0026gt;{escape(description)}\u0026lt;/strong\u0026gt;\u0026lt;/div\u0026gt;  \u0026lt;/div\u0026gt;  ...  \u0026lt;/section\u0026gt;  As there is only one generated file and classes are global, it can be imported pretty much anywhere in your code as long it is included in your bundle.\n Serve your solution using npm run serve et voilà!:  Advanced scenarios Deal with the dark mode In components, you often need to handle both light and dark modes according to the host theme selected (ex: on a SharePoint site). It means CSS classes used have to be aware of colors from the theme that should be applied. Despite TailwindCSS has its own way to handle dark mode, we can\u0026rsquo;t really rely on it with the SharePoint Framework (we could but this is not optimal in my opinion). By default, there is no \u0026ldquo;link\u0026rdquo; between TailwindCSS classes and SPFx theme values. We have to create one.\nThe beauty of TailwindCSS is it is very customizable. To create that \u0026rsquo;link\u0026rsquo; we simply extend the default theme definition in the TailwindCSS tailwind.config.js configuration file and use custom CSS variables for common colors (ex: body text, primary color, etc.):\n{  ...  theme: {  extend: {  fontFamily: {  sans: [\u0026#39;var(--myWebPart-fontPrimary)\u0026#39;,\u0026#39;Roboto\u0026#39;, ...defaultTheme.fontFamily.sans]  },  colors: {  primary: \u0026#34;var(--myWebPart-primary, #7C4DFF)\u0026#34;,  background: \u0026#34;var(--myWebPart-background, #F3F5F6)\u0026#34;,  link: \u0026#34;var(--myWebPart-link, #1E252B)\u0026#34;,  linkHover: \u0026#34;var(--myWebPart-linkHover, #1E252B)\u0026#34;,  bodyText: \u0026#34;var(--myWebPart-bodyText, #1E252B)\u0026#34;  }  }  } } These CSS variables are then set in the component according to the current theme values:\n// All CSS variables from my project export enum ThemeCSSVariables {  fontFamilyPrimary = \u0026#34;--myWebPart-fontPrimary\u0026#34;,  colorPrimary = \u0026#34;--myWebPart-primary\u0026#34;,  background = \u0026#34;--myWebPart-background\u0026#34;,  primaryBackgroundColorDark = \u0026#34;--myWebPart-colorBackgroundDarkPrimary\u0026#34;,  bodyText= \u0026#34;--myWebPart-bodyText\u0026#34;,  link = \u0026#34;--myWebPart-link\u0026#34;,  linkHover = \u0026#34;--myWebPart-linkHover\u0026#34;, }  ... // In the main WebPart class protected onThemeChanged(currentTheme: IReadonlyTheme | undefined): void {  if (!currentTheme) {  return;  }   this._isDarkTheme = !!currentTheme.isInverted;  const {  semanticColors  } = currentTheme;   if (semanticColors) {   // Map colors from the theme to CSS variables and therefore to TailwindCSS custom colors so they can be used in classes  this.domElement.style.setProperty(ThemeCSSVariables.colorPrimary, currentTheme?.palette?.themePrimary || null);  this.domElement.style.setProperty(ThemeCSSVariables.fontFamilyPrimary, currentTheme?.fonts?.medium?.fontFamily || null);  this.domElement.style.setProperty(ThemeCSSVariables.bodyText, semanticColors.bodyText || null);  this.domElement.style.setProperty(ThemeCSSVariables.link, semanticColors.link || null);  this.domElement.style.setProperty(ThemeCSSVariables.linkHover, semanticColors.linkHovered || null);  this.domElement.style.setProperty(ThemeCSSVariables.background, semanticColors.bodyBackground || null);  } } Then in components you can use the custom colors like this:\n\u0026lt;section className=\u0026#39;text-bodyText bg-background font-sans\u0026#39;\u0026gt;  ... \u0026lt;/section\u0026gt; Is there any drawbacks to use TailwindCSS with SPFx? Unfortunately yes, nothing is perfect! These are more considerations to have when you choose to use TailwindCSS over traditional SASS/CSS stylesheets. Among them:\n As TailwindCSS generates a big CSS stylesheet in the end, there is no more stylesheets isolation (ex: one stylesheet per component as best practice). It can be harder to quickly identify which component uses what classes if you defined a lot of custom classes via theme extentsions. If you have multiple Web Parts or extensions on the same page built from different projects/solutions independently, you will end up with duplicated declarations for CSS classes as each solution will generate its own stylesheet. It is not that critical but not optimal regarding the bundle sizes. Also there is no really a risk of name conflicts as class names are standardized. The only conflicts you could have is if other solutions defined the same custom extensions. To mitigate this point:  Try to build Web Parts or extensions from the same project using the same TailwindCSS configuration file. Avoid using different version of TailwindCSS across solutions.    Conclusion I hope you found this post useful and you will be curious to give it a try in your solutions. To me, TailwindCSS was a game changer in the way I work with the SharePoint Framework and even beyond. It saved me a lot of time in my projects and also probably improved my mental heath to not being forced to write CSS code anymore :D.\nBe careful: once you start to use it, you can\u0026rsquo;t go back 😁\n","date":"March 27, 2024","image":null,"permalink":"/post/using-tailwindcss-with-spfx/","title":"Use TailwindCSS with SPFx"},{"categories":["modern-search"],"contents":"Since the 4.6.0 version of the PnP Modern Search solution, you can now configure sort fields for SharePoint/Microsoft Search data sources for both default sort and user sort (i.e sort results from the UI). As a result the previous \u0026ldquo;Edit sort order\u0026rdquo; option for data sources has been renamed to \u0026ldquo;Edit sort settings\u0026rdquo;.\nThat was a long awaited feature already present in v3 but not migrated to v4 due to the new architecture. This is now fixed :D.\nWhat changes compared to previous versions? The first difference regarding the configuration is about the properties listed in the sort fields dropdown. Now the list is a static list representing all \u0026ldquo;Sortable\u0026rdquo; properties in the SharePoint search schema:\nHowever, if a property is missing or you need to use a custom property like RefinableString or RefinableDate, you can sill type the property name manually in the dropdown and press \u0026lsquo;Enter\u0026rsquo; to validate. Remember the property should be \u0026ldquo;Sortable\u0026rdquo; in the search schema to get it work as there is no error validation anymore.\nThen you can now decide for a property to be used either for default sorting or user sorting or both. For instance if you want to sort the results initialy by the Created managed property in ascending direction and also allow users to change this direction afterwards, you can set this property as user sort as well with a friendly name for users (i.e. the name that will appeart in the sort dropdown). You can also set other properties to only be sortable in the UI but not as default, like RefinableString01:\nOnce an user sort property is added in the configuration, a sort drop down control appears in the layout (if you don\u0026rsquo;t set any \u0026lsquo;User sort\u0026rsquo; property, the control does not appear):\n The dropdown control appears on all layouts (except the \u0026lsquo;Details List\u0026rsquo;)\n To sort results on a property, simply click on the property you want to sort in the list (1) and then select the direction (2) ascending or descending.\nHowever, here are few things to know about the sort control:\n  By default, when the page opens, the sort order will be the one represented by all properties set as \u0026lsquo;Default sort\u0026rsquo; in the configuration (all combined together according to the definition order).\n  You can\u0026rsquo;t preselect a property in the sort dropdown even if it is configured as a default sort property. It means users don\u0026rsquo;t kow what is the default sort order.\n  Although you can set multiple properties as user sort in the configuration, you can sort on only one property at a time in the UI.\n  The default sort direction when you click on a property is always ascending (and you can\u0026rsquo;t change this).\n  You can reset the default sort order by clicking on the \u0026ldquo;default\u0026rdquo; option:\n  Details list layout specificity We\u0026rsquo;ve also updated the Details List layout sort options to match this new behavior as it contains its own sort settings. Now in the column option you can make a specific column sortable by choosing among the properties you configured as \u0026lsquo;User sort\u0026rsquo; in the data source sort configuration (and only these ones):\nIn the update process, we\u0026rsquo;ve also removed the static sorting behavior since it was too confusing for users. Now when you sort a specific column, all resuts in the data set are sorted, not only the current page (i.e. a new search request is made):\nHope you will find this new feature useful! Don\u0026rsquo;t hesitate to provide your feedback in the GitHub repository.\n","date":"April 8, 2022","image":null,"permalink":"/post/modern-search-overview-sort-control/","title":"New sort feature with the 4.6.0 PnP Modern Search version!"},{"categories":["general"],"contents":"The new SharePoint Online content type hub is a really geat feature to ensure a standardized information architecture across the company. Compared to the previous version, a lot of improvements has been made, especially the synchronization process with consumer sites which is now much faster and convenient.\nHowever, to be able to add content types in a library or list coming from the hub in a specific SharePoint site, you must first synchronize them. Otherwise you will get a Not found error during the addition.\nIf like me, you work with the content type hub programmatically to provide site templates, here are some ways to do that depending your implementation strategy:\nSynchronize with CSOM and REST (example with a Logic App) Unfortunately, and as far as I know, there is no Microsoft Graph REST endpoint that can be used to do this synhronization for a single site. However, you can still use the SyncContentTypesFromHubSite2 CSOM method over the /_vti_bin/client.svc/ProcessQuery endpoint directly with the following XML payload (example):\n\u0026lt;Request xmlns=\u0026#34;http://schemas.microsoft.com/sharepoint/clientquery/2009\u0026#34; AddExpandoFieldTypeSuffix=\u0026#34;true\u0026#34; SchemaVersion=\u0026#34;15.0.0.0\u0026#34; LibraryVersion=\u0026#34;16.0.0.0\u0026#34; ApplicationName=\u0026#34;.NET Library\u0026#34;\u0026gt;  \u0026lt;Actions\u0026gt;  \u0026lt;Method Name=\u0026#34;SyncContentTypesFromHubSite2\u0026#34; Id=\u0026#34;107\u0026#34; ObjectPathId=\u0026#34;104\u0026#34;\u0026gt;  \u0026lt;Parameters\u0026gt;  \u0026lt;Parameter Type=\u0026#34;String\u0026#34;\u0026gt;https://yourtenant.sharepoint.com/sites/yoursite\u0026lt;/Parameter\u0026gt;  \u0026lt;Parameter Type=\u0026#34;Array\u0026#34;\u0026gt;  \u0026lt;Object Type=\u0026#34;String\u0026#34;\u0026gt;0x0101009679330D8335B9448FC3FFCD592FF7B3\u0026lt;/Object\u0026gt;  \u0026lt;Object Type=\u0026#34;String\u0026#34;\u0026gt;0x0101009D1CB255DA76424F860D91F20E6C411800DBFEC5E8F9D780498850457DA36850BE\u0026lt;/Object\u0026gt;  \u0026lt;/Parameter\u0026gt;  \u0026lt;/Parameters\u0026gt;  \u0026lt;/Method\u0026gt;  \u0026lt;/Actions\u0026gt;  \u0026lt;ObjectPaths\u0026gt;  \u0026lt;Constructor Id=\u0026#34;104\u0026#34; TypeId=\u0026#34;{618709d0-5c34-4b0a-bd15-0406f9e62cc2}\u0026#34; /\u0026gt;  \u0026lt;/ObjectPaths\u0026gt; \u0026lt;/Request\u0026gt;  From here, it is easy to build the parameters array with content type IDs to synchronize (ex: synchronize only a specific content type group in the hub):\n The XML payload shouldn\u0026rsquo;t contains any spaces.\n  The HTTP header Content-Type needs be set to \u0026quot;text/xml\u0026quot;.\n  The call with always return an HTTP 200 even if it does not succeed. To get the error details, you will need to parse the response manually.\n Use the buitlin addContentTypesFromHub action in a site script If you are using site designs/templates, you can also rely on the builtin addContentTypesFromHub action:\n{  \u0026#34;$schema\u0026#34;: \u0026#34;schema.json\u0026#34;,  \u0026#34;actions\u0026#34;: [  {  \u0026#34;verb\u0026#34;: \u0026#34;addContentTypesFromHub\u0026#34;,  \u0026#34;ids\u0026#34;: [\u0026#34;0x01007CE30DD1206047728BAFD1C39A850120\u0026#34;]  }  ],  \u0026#34;bindata\u0026#34;: {},  \u0026#34;version\u0026#34;: 1 } Use PnP PowerShell Last but not least, you can use the PnP.Powershell Add-PnPContentTypesFromContentTypeHub cmdlet. Basically it uses CSOM and SyncContentTypesFromHubSite2 behind the scenes.\nAdd-PnPContentTypesFromContentTypeHub -ContentTypes \u0026#34;0x010057C83E557396744783531D80144BD08D\u0026#34; -Site https://tenant.sharepoint.com/sites/HR ","date":"March 30, 2022","image":null,"permalink":"/post/synchronize_content_types_from_hub/","title":"Programmatically synchronize content types to a site from content type hub"},{"categories":["modern-search"],"contents":"In the latest release of PnP Modern Search (4.5.3), we\u0026rsquo;ve added the ability to connect two Search Results Web Parts together. It went quite unnoticed at the time but it actually unlocks a lot of cool new scenarios requiring dynamic filtering, for instance, to build dynamic dashboards experiences. In this article I will review how this works and how you can benefit from this feature for your search pages.\nHow to connect two search results Web Parts together? Prerequisite: understand how dynamic filering works When you connect two Web Parts together, you basically match a value from a selected source item property against a value contained in a property target items. In other terms, when an item is selected:\n \u0026ldquo;Retrieve all items where the property X contains the value of the property Y of the selected item\u0026rdquo;\n Notice that properties don\u0026rsquo;t need to be necessarily the same in both sides as two different properties can share common values. You can already experience this configuration with the dynamic filtering feature for List and Library default Web Parts:\nThe concept is the same with PnP Search Results Web Parts, except you get more flexibility on how the query is made.\nExample with a taxonomy column To illustrate the item selection feature between two Search Results Web Parts, I take the example of a devices catalog that should display the related documentation when the user select one or more device from the list:\nTo make the link between items, the two lists share a same managed metadata column named \u0026ldquo;Related product\u0026rdquo;:\nBeacuse, we use SharePoint search, I also created a corresponding managed property in the search schema using the term ID property (not the text one):\nThe Search Results Web Parts connection configuration is always done in two steps.\n1- Allow item selection in source Web Part The first step is to configure a source Web Part where items will be selected and then enable the item selection feature in the layout configuration page under Common options:\n Item selection is available for all layouts.\n You can also turn on/off multiple selection for results.\n2- Configure connection on target Web Part(s) The second step is to configure the connection in the target Web Parts, the one(s) where results should be filtered by selected items of the source one. In my case the documents related to a specific product. To achieve this, I add a new Web Part on the page and configure a base query to retrieve documents that will be filtered:\n You can connect multiple target Web Parts to a single source.\n The connection with the source Search Results Web Part is done on the third configuration page in the property pane. From here you have to set:\n The source search Results Web Part to use. If you select anything else than a search results Web Part, the configuration is simply reset. The property of the selected item containing the value to use as filter. In my case, I select the managed property name corresponding to my column. The property of items used to match the selected value. In my case, because the two lists share the same column, I can simply use the same managed property name (RefinableString04).   To see the list of the available values, the source/target Web Part must have results retrieved and properties added in the selected properties in the data source.   The selection mode. By default the filtering is done using refinement filters just like if you\u0026rsquo;ve selected a value from the Search Filters Web Part. However, this mode has two constraints:\n The target Web Part must display results initially (because you can\u0026rsquo;t refine empty results basically:/). The target property (i.e. destination field name) must be a refinable managed property in the search schema. It won\u0026rsquo;t work otherwise.    If you\u0026rsquo;ve enabled mutli selection in the source Web Part, the operator you want to use between selected values (AND/OR).\n  After doing these configurations, you should be able to filter (refine actually) values in the target Web Part.\nUse the manual selection mode The default selection mode constraints you to display a set of results at page load in target Web Parts which is, in some cases, not the behavior you neccessarily want in terms of UI experience. To bypass this limitation, a manual selection mode is available. When enabled, it is up to you to configure the query using the query template field in the data source and available tokens:\nQuery template in manual mode\n{? Path:{site.absoluteUrl}/Documentation {|RefinableString04:{filters.RefinableString04.valueAsText}} IsDocument:1}\nExplanation:\n {? ... }: Conditional operator. If a token is not resolved in the enclosed expression, the query won\u0026rsquo;t be submitted to the search engine (meaning you won\u0026rsquo;t get any default results). {| ... }: \u0026lsquo;OR\u0026rsquo; operator token that will expand the conditions with the OR KQL operator. Use {\u0026amp; ... } if you want and AND condition. {filters.RefinableString04.valueAsText}: Token that will be replaced dynamically by the selected item value. Only string values are supported here.  With this configuration, you won\u0026rsquo;t get any result at page load but only when you will select an item in the source Web Part. To completely hide target Web Part(s), you can also enable the \u0026quot;\u0026quot; option.\nConnection with native SharePoint \u0026lsquo;List\u0026rsquo; Or \u0026lsquo;Library\u0026rsquo; Web Parts The cool thing about the item selection feature is it supports connection with OOTB SharePoint Web Parts. It means you can configure a \u0026lsquo;List\u0026rsquo; or \u0026lsquo;Library\u0026rsquo; Web Part as source and use the PnP Search Results Web Part(s) as targets.\n OOTB SharePoint Web Parts can only be used as source, not targets.\n In my previous example, I could totally substitute the PnP Search Results displaying devices from he list by the list itself using hte \u0026lsquo;List\u0026rsquo; Web Part. Also, because I\u0026rsquo;ve mapped the \u0026lsquo;Related Product\u0026rsquo; managed metadata column to the property RefinableString04 using the ows_taxId_\u0026lt;columnName\u0026gt;, I can perform the match based on the taxonomy term ID. Simple as that. The other configurations remaisn exactly the same, only the source changes.\nThis type of connection supports single and multi value selection as well:\nConclusion The new item selection feature of PnP Modern Search solution now unlocks many new scenarios to build amazing dynamic search experiences. Looking forward to see how the community will use it!\nFor more information:\n Item selection feature documentation PnP Modern Search 4.5.4  ","date":"February 13, 2022","image":null,"permalink":"/post/modern-search-focus-item-selection/","title":"PnP Modern Search: focus on item selection feature"},{"categories":["general"],"contents":"Remember thecollaborationcorner.com? Wel\u0026hellip;it\u0026rsquo;s been quite a while since my last blog post. Actually I closed my previous blog (thecollaborationcorner.com) almost 3 years ago due to a lack of time and motivation. After a quite busy period for me since the last two past years (two new born in 3 years!), I finally decided to go forward and open a blog again, but this time using cool new tools. The last one was built with Wordpress and became quite difficult to manage through years. That\u0026rsquo;s why I chose Hugo to do the job this time.\n     I was first hesitating with Jekyll at some point but finally found Hugo way easier and cleaner to use, at least for me, especially the setup with GitHub pages.\nWhat you\u0026rsquo;ll find on this blog? For those who know me personally, they probably know I\u0026rsquo;m not a huge fan of social medias, unlike some people, I don\u0026rsquo;t write content just for writing content to maintain my \u0026ldquo;online presence\u0026rdquo;. I\u0026rsquo;ve just needed a space to write down my ideas, solutions and other articles related, of course, mainly to the PnP Modern Search activity, but also other topics like Azure, Microsoft 365 and software development practices in general!\nI hope you will enjoy this new blog and I wish you a good year 2022!\nFranck.\n","date":"January 29, 2022","image":null,"permalink":"/post/new-year-new-blog/","title":"New year, new blog!"},{"categories":["modern-search"],"contents":" Finally. It\u0026rsquo;s been a while I wanted to create this content but now it is live! I\u0026rsquo;m glad to announce I\u0026rsquo;ve released a complete course about usage and configuration for the PnP Modern Search solution. This course arrives with the latest 4.5.4 version and should help you to use the solution at its full potential.\nThis course is divided in five main chapters, one for the installation, one for each Web Part and finally, end-to-end tutorials about business scenarios.\n Get started with the PnP Modern Search Solution Use the Search Results Web Part Use the Search Filters Web Part Use the Search Box Web Part Use the Search Verticals Web Part Let\u0026rsquo;s practice!  With these you get near of 3 hours of content reviewing Web Parts configuration.\nCourse player experience  Get Started!   Launch offer Get a 20% off until Feb 28 2022 with the code PNPSEARCH2022\n Regularly updated content This course is an evolving material. Some lessons or chapters may change or be updated in the future as long the solution evolves and/or according your feedback. By getting this course, you have access to new content and updates for free.\nPay attention to the used version and lesson description For all lessons, the used version of the PnP Modern Search is always mentionned in the lesson description. Make sure you are using the same version to ensure the same experience. Some features in lessons are quite new and not necessarily available for previous versions.\nAlso, key notions are always summarized in the description to help to quickly identify important things to know.\nGet some help when needed If you get stuck with something or don\u0026rsquo;t understand a configuration presented in a lesson, you can still start the discussion using the button on the top right corner.\nI will be more than happy to provide you some answers an guidance. With this course, you have a privileged channel with the main author so use it when needed.\nIf you encounter a bug or you a not able to reproduce behavior demonstrated in a lesson, you can still open a new issue on GitHub . We will review it sooner as we can.\nYour feedback is important! Actually, as a PnP Modern Search user you are the best person to provide your feedback about features and usability. By doing this, you help me to build a better course and a better solution for the community.\nI also plan to release an other course about all customizations scenarios you can have with the PnP Modern Search but later this year as it take quite some time to build.\n","date":"January 29, 2022","image":null,"permalink":"/post/new-pnp-modern-course/","title":"PnP Modern Search course is now available!"},{"categories":null,"contents":"H1 Heading H2 Heading H3 Heading H4 Heading H5 Heading H6 Heading  Paragraph Did you come here for something in particular or just general Riker-bashing? And blowing into maximum warp speed, you appeared for an instant to be in two places at once. We have a saboteur aboard. We know you’re dealing in stolen ore. But I wanna talk about the assassination attempt on Lieutenant Worf. Could someone survive inside a transporter buffer for 75 years? Fate. It protects fools, little children, and ships.\n Emphasis :  Did you come here for something in particular or just general Did you come here for something in particular Did you come here Did you come here for something in particular Did you come here for something in particular  Did you come here for something in particular URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).   Ordered list  you appeared for an instant to be in two places at once. We have a saboteur aboard. you appeared for an instant to be in two places at once.   Unordered list  Quisque sem ipsum, placerat nec tortor vel, blandit vestibulum libero. Morbi sollicitudin viverra justo Blandit vestibulum libero. Morbi sollicitudin viverra justo Placerat nec tortor vel, blandit vestibulum libero. Morbi sollicitudin viverra justo   Code and Syntax Highlighting : var s = \u0026#34;JavaScript syntax highlighting\u0026#34;; const plukDeop = key =\u0026gt; obj =\u0026gt; key.split const compose = key =\u0026gt; obj =\u0026gt; key.split alert(s); var s = \u0026#34;JavaScript syntax highlighting\u0026#34;; const plukDeop = key =\u0026gt; obj =\u0026gt; key.split const compose = key =\u0026gt; obj =\u0026gt; key.split alert(s);  Buttons Button  Quote  “Did you come here for something in particular or just general Riker-bashing? And blowing into maximum warp speed, you appeared for an instant to be in two places at once.”\n  Notice : This is a simple note.\n This is a simple tip.\n This is a simple info.\n This is a simple warning.\n  Tab :  Title goes here Did you come here for something in particular or just general Riker-bashing? And blowing into maximum warp speed, you appeared for an instant to be in two places at once. We have a saboteur aboard. We know you’re dealing in stolen ore. But I wanna talk about the assassination attempt on Lieutenant Worf.  Title goes here Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.  Title goes here Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo     Table :    # First Last Handle     1 Row:1 Cell:1 Row:1 Cell:2 Row:1 Cell:3   2 Row:2 Cell:1 Row:2 Cell:2 Row:2 Cell:3   3 Row:3 Cell:1 Row:3 Cell:2 Row:3 Cell:3     Collapse : collapse 1    Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur    collapse 2    Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur    collapse 3    Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur adipisicing elit. Lorem ipsum dolor sit amet consectetur     Image https://blog.franckcornu.com/images/post/post-2.jpg does not exist  Youtube :   ","date":"January 1, 1","image":null,"permalink":"/elements/","title":"Elements"},{"categories":null,"contents":"I\u0026rsquo;ve been SharePoint \u0026amp; Microsoft 365 consultant for almost 10 years now and Office Development MVP, I’ve worked for several companies in many fields especially intranet solutions. Involved in the Pattern \u0026amp; Practices initiative, I’m the main author of the PnP Modern Search solution.\nYou won\u0026rsquo;t find any better source of information than me about this solution ;)\n","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/author/franck-cornu/","title":"Franck Cornu"},{"categories":null,"contents":"I\u0026rsquo;ve been SharePoint \u0026amp; Microsoft 365 consultant for almost 10 years now and Office Development MVP, I’ve worked for several companies in many fields especially intranet solutions. Involved in the Pattern \u0026amp; Practices initiative, I’m the main author of the PnP Modern Search solution.\nYou won\u0026rsquo;t find any better source of information than me about this solution ;)\n     De tous ceux qui n\u0026rsquo;ont rien à dire les plus agréables sont ceux qui se taisent\n-Coluche (french humorist)   ","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/about/","title":"Hello there, I am Franck Cornu"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/homepage/home/","title":"Homepage"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/homepage/full/","title":"Homepage Full"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/homepage/full-left/","title":"Homepage Full Left"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/homepage/full-right/","title":"Homepage Full Right"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/homepage/grid/","title":"Homepage Grid"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/homepage/grid-left/","title":"Homepage Grid Left"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/homepage/grid-right/","title":"Homepage Grid Right"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/homepage/list/","title":"Homepage List"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/homepage/list-left/","title":"Homepage List Left"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/homepage/list-right/","title":"Homepage List Right"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/404/","title":"No Search Found"},{"categories":null,"contents":"","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/search/","title":"Search Results"},{"categories":null,"contents":"Ask Us Anything Or just Say Hi, Rather than just filling out a form, Sleeknote also offers help to the user with links directing them to find additional information or take popular actions.\n","date":"January 1, 1","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cimg loading=\"lazy\" decoding=\"async\" src=\"/images/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_q90_h2_box_3.webp\" alt=\"\" class=\"img-fluid\" width=\"650\" height=\"\" onerror=\"this.onerror='null';this.src='\\/images\\/author_hu9b02e0e5fa61fa4710f0e85bb0f4dbed_853600_650x0_resize_box_3.png'\"\u003e\n \n \n \n\n","permalink":"/contact/","title":"Talk To Me Anytime :)"}]